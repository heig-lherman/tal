{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://heig-vd.ch/docs/default-source/doc-global-newsletter/2020-slim.svg\" alt=\"HEIG-VD Logo\" width=\"100\" align=\"right\" /> \n",
    "\n",
    "# Cours TAL - Laboratoire 2<br/>*POS taggers* pour le français dans spaCy et NLTK\n",
    "\n",
    "**Objectif**\n",
    "\n",
    "Comparer l'étiqueteur morphosyntaxique français prêt-à-l'emploi de spaCy avec deux étiqueteurs entraînés, l'un dans spaCy et l'autre dans NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation et test de spaCy\n",
    "\n",
    "La boîte à outils spaCy est une librairie Python *open source* pour le TAL, dédiée à un usage en production. Les documents suivants vous seront utiles :\n",
    "* comment [installer](https://spacy.io/usage) spaCy\n",
    "* comment [télécharger un modèle](https://spacy.io/usage/models) pour une langue donnée (on appelle ces modèles des *trained pipelines* car ils enchaînent plusieurs traitements)\n",
    "* comment faire les [premiers pas](https://spacy.io/usage/spacy-101) dans l'utilisation de spaCy\n",
    "\n",
    "Veuillez installer spaCy, puis la *pipeline* pour le français appelée `fr_core_news_sm`.  Si vous utilisez *conda*, installez spaCy dans l'environnement du cours TAL."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:46:34.834059Z",
     "start_time": "2025-03-16T18:46:31.320400Z"
    }
   },
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\") # charge la pipeline\n",
    "import tqdm # permet l'affichage d'une barre de progression"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1a.** Une pipeline effectue un ensemble de traitements d'un texte en lui ajoutant des annotations.  Les traitements effectués par la pipeline `fr_core_news_sm` sont [documentés ici](https://spacy.io/models/fr#fr_core_news_sm).  La liste des traitements d'une pipeline figure dans son attribut `.pipe_names`.  On peut activer ou désactiver un traitement T avec, respectivement, les méthodes `.disable_pipe(T)` et `.enable_pipe(T)` appliquées à la pipeline.\n",
    "\n",
    "* Veuillez afficher les traitements disponibles dans la pipeline `fr_core_news_sm` chargée ci-dessus sous le nom de `nlp` .\n",
    "* Veuillez désactiver tous les traitements sauf `tok2vec` et `morphologizer` (on fait cela pour accélerer le traitement).\n",
    "* Vérifiez que la désactivation a bien fonctionné en affichant les traitements activés."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:46:34.850910Z",
     "start_time": "2025-03-16T18:46:34.846759Z"
    }
   },
   "source": [
    "print(f\"Liste des traitements disponibles : {nlp.pipe_names}\")\n",
    "\n",
    "for pipe in nlp.pipe_names:\n",
    "    if pipe != \"tok2vec\" and pipe != \"morphologizer\":\n",
    "        nlp.disable_pipe(pipe)\n",
    "\n",
    "print(f\"Liste des traitements actifs : {nlp.pipe_names}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des traitements disponibles : ['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "Liste des traitements actifs : ['tok2vec', 'morphologizer']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:46:35.013331Z",
     "start_time": "2025-03-16T18:46:35.010467Z"
    }
   },
   "source": [
    "from spacy.lang.fr.examples import sentences"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1b.** L'objet `sentences` chargé ci-dessus contient une liste de phrases en français. \n",
    "\n",
    "* Veuillez afficher les deux premières phrases de `sentences`.\n",
    "* Veuillez analyser chacune de ces deux phrases avec la pipeline `nlp` puis afficher chaque token et son POS tag.\n",
    "    * indication : aidez-vous de la [documentation](https://spacy.io/models/fr#fr_core_news_sm) de `fr_core_news_sm`\n",
    "    * consigne d'affichage : indiquer le tag entre crochets après chaque token, comme ceci : Les \\[DET\\] robots \\[NOUN\\] ...\n",
    "    * note : la documentation détaillée du POS tagging dans spaCy est [disponible ici](https://spacy.io/usage/linguistic-features)\n",
    "* Veuillez commenter la tokenisation et les POS tags observés : vous semblent-ils corrects pour les deux phrases ?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:46:35.092127Z",
     "start_time": "2025-03-16T18:46:35.071873Z"
    }
   },
   "source": [
    "def print_sentences_token_with_tags(s, print_sentence=True):\n",
    "    if print_sentence :\n",
    "        print(f\"Phrase : {s}\")\n",
    "\n",
    "    tokens = nlp(s)\n",
    "    res = [f\"{token.text} [{token.pos_}]\" for token in tokens]\n",
    "    print(' '.join(res))\n",
    "\n",
    "print_sentences_token_with_tags(sentences[0])\n",
    "print(\"\\n\")\n",
    "print_sentences_token_with_tags(sentences[1])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase : Apple cherche à acheter une start-up anglaise pour 1 milliard de dollars\n",
      "Apple [NOUN] cherche [NOUN] à [ADP] acheter [VERB] une [DET] start [NOUN] - [NOUN] up [ADJ] anglaise [NOUN] pour [ADP] 1 [NUM] milliard [NOUN] de [ADP] dollars [NOUN]\n",
      "\n",
      "\n",
      "Phrase : Les voitures autonomes déplacent la responsabilité de l'assurance vers les constructeurs\n",
      "Les [DET] voitures [NOUN] autonomes [ADJ] déplacent [ADV] la [DET] responsabilité [NOUN] de [ADP] l' [DET] assurance [NOUN] vers [ADP] les [DET] constructeurs [NOUN]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Avec ces deux phrases, le tagging semble relativement bon, à quelques exceptions près. Sachant que nous utilisons ici la version `small` du modèle, il n'est pas surprenant que quelques erreurs soient présentes.\n",
    "> Les problèmes obervés sont les suivants :\n",
    "> - Phrase 1 :\n",
    ">     - `cherche` : Le verbe est considéré comme étant un nom\n",
    ">     - `start-up` : Le nom composé est séparé en trois parties, prenant `-` comme un nom à part entière et `up` comme un adjectif. \n",
    ">     - `anglaise` : L'adjectif est considéré comme étant un nom. Cette erreur provient probalement du fait que `start-up` est mal calssifié et donc que l'adjectif n'a pas pu être reconnu comme tel.\n",
    ">  - Phrase 2 :\n",
    ">     - `déplacent` : Le verbe est considéré comme étant un adverbe\n",
    ">\n",
    "> Le cas de `start-up` fait penser à l'utilisation d'une valeur par défaut (`NOUN`) pour les mots inconnus et une difficulté lors de la tokenization à prendre en compte les mots composés. Etant donné que `start-up`est un anglicisme et que nous n'avons pas d'autres mots composés dans ces deux phrases, cela pourrait ne conserner que les mots composés ne faisant pas partie du vocabulaire français."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prise en main des données\n",
    "\n",
    "Les données sont fournies dans un format tabulaire dans l'archive `UD_French-GSD.zip` sur Cyberlearn.  Elles sont basées sur les données fournies par le projet [Universal Dependencies](https://github.com/UniversalDependencies/UD_French-GSD).  Leur format, appelé CoNLL-U, est [documenté ici](https://universaldependencies.org/format.html).  Veuillez placer les trois fichiers contenus dans l'archive dans un sous-dossier de ce notebook nommé `spacy_data`.\n",
    "\n",
    "Les trois fichiers contiennent des phrases en français annotées avec les POS tags :\n",
    "* le fichier `fr-ud-train.conllu` est destiné à l'entraînement\n",
    "* le fichier `fr-ud-dev.conllu` est destiné aux tests préliminaires et aux réglages des paramètres\n",
    "* le fichier `fr-ud-test.conllu` est destiné à l'évaluation finale.\n",
    "\n",
    "**2a.** En inspectant les fichiers avec un éditeur texte, veuillez déterminer dans quelle colonne se trouvent les *tokens* des textes originaux, et dans quelle colonne se trouvent leurs étiquettes morpho-syntaxiques correctes (*POS tags*).  Que contient la troisième colonne ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> En inspectant les fichiers, on observe que :\n",
    "> - la première colonne contient la position du token dans la phrase\n",
    "> - la deuxième colonne contient le token de la phrase originale\n",
    "> - la troisième colonne contient le lemme associé au token\n",
    "> - la quatrième colonne contient l'étiquette morpho-syntaxique correcte"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b.** Veuillez convertir les trois fichiers de données en des fichiers binaires utilisables par spaCy, en utilisant la [commande 'convert' fournie par spaCy](https://spacy.io/api/cli#convert).  La commande est donnée ci-dessous, le premier dossier `./input_data` contient les 3 fichiers `.conllu` et le dossier `./spacy-data` contiendra les 3 résultats.\n",
    "\n",
    "* Veuillez exécuter la commande de conversion.\n",
    "* Combien de phrases environ (à 10 phrases près) contient chaque fichier (*train*, *dev*, *test*) ?  Observez la commande et son résultat pour répondre."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:46:55.454794Z",
     "start_time": "2025-03-16T18:46:35.188769Z"
    }
   },
   "source": [
    "!python -m spacy convert ./input_data ./spacy_data --converter conllu  --n-sents 10 --lang fr"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;4mℹ Grouping every 10 sentences into a document.\u001B[0m\r\n",
      "\u001B[38;5;2m✔ Generated output file (148 documents): spacy_data/fr-ud-dev.spacy\u001B[0m\r\n",
      "\u001B[38;5;4mℹ Grouping every 10 sentences into a document.\u001B[0m\r\n",
      "\u001B[38;5;2m✔ Generated output file (42 documents): spacy_data/fr-ud-test.spacy\u001B[0m\r\n",
      "\u001B[38;5;4mℹ Grouping every 10 sentences into a document.\u001B[0m\r\n",
      "\u001B[38;5;2m✔ Generated output file (1456 documents):\r\n",
      "spacy_data/fr-ud-train.spacy\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> La commande va grouper 10 phrases dans un même document. Étant donné le résultat affiché ci-dessus, nous pouvons donc affirmer que les fichiers contiennent, à 10 phrases près :\n",
    "> - train : 14560 phrases   (soit 1456 documents * 10 phrases par document) \n",
    "> - dev   :  1480 phrases   (soit  148 documents * 10 phrases par document) \n",
    "> - test  :   420 phrases   (soit   42 documents * 10 phrases par document) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2c**. Les données des fichiers convertis peuvent être chargées dans un objet de type `DocBin`.  Dans notre cas, un tel objet contient un ensemble de documents, chacun contenant 10 phrases.  Chaque document est un objet de type `Doc`.  Le code donné ci-dessous vous permet de charger les données de test et vous montre comment les afficher.\n",
    "\n",
    "* Veuillez stocker la première phrase des données de test dans une variable nommée `premiere_phrase_test`.\n",
    "* Veuillez afficher cette phrase, ainsi que son type dans spaCy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:46:55.476484Z",
     "start_time": "2025-03-16T18:46:55.468902Z"
    }
   },
   "source": [
    "from spacy.tokens import DocBin\n",
    "from spacy.tokens import Doc\n",
    "test_data = DocBin().from_disk(\"./spacy_data/fr-ud-test.spacy\")\n",
    "# Exemple d'utilisation (afficher toutes les phrases)\n",
    "# for doc in test_data.get_docs(nlp.vocab): \n",
    "#     for sent in doc.sents:\n",
    "#         print(sent)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:46:55.580753Z",
     "start_time": "2025-03-16T18:46:55.515009Z"
    }
   },
   "source": [
    "first_doc = next(test_data.get_docs(nlp.vocab))\n",
    "premiere_phrase_test = next(first_doc.sents)\n",
    "\n",
    "print(f\"Première phrase : {premiere_phrase_test}\")\n",
    "print(f\"\\nType de la première phrase : {type(premiere_phrase_test)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Première phrase : Je sens qu'entre ça et les films de médecins et scientifiques fous que nous avons déjà vus, nous pourrions emprunter un autre chemin pour l'origine.\n",
      "\n",
      "Type de la première phrase : <class 'spacy.tokens.span.Span'>\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Évaluation du POS tagger français de la pipeline `fr_core_news_sm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3a.** Veuillez effectuer le *POS tagging* avec spaCy de la `premiere_phrase_test` et afficher les résultats dans le format demandé au (1b).  Indication : convertissez la `premiere_phrase_test` dans un objet de type `Doc` en lui appliquant la méthode `.as_doc()`.  Cet objet peut être ensuite traité par la pipeline `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:46:55.621060Z",
     "start_time": "2025-03-16T18:46:55.606112Z"
    }
   },
   "source": [
    "sentence_doc = nlp(premiere_phrase_test.as_doc().text)\n",
    "\n",
    "print(\"Résultat de la pipeline sur la première phrase:\")\n",
    "print_sentences_token_with_tags(sentence_doc, False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat de la pipeline sur la première phrase:\n",
      "Je [PRON] sens [VERB] qu' [SCONJ] entre [ADP] ça [PRON] et [CCONJ] les [DET] films [NOUN] de [ADP] médecins [NOUN] et [CCONJ] scientifiques [NOUN] fous [PRON] que [PRON] nous [PRON] avons [AUX] déjà [ADV] vus [VERB] , [PUNCT] nous [PRON] pourrions [VERB] emprunter [VERB] un [DET] autre [ADJ] chemin [NOUN] pour [ADP] l' [DET] origine [NOUN] . [PUNCT]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3b.** Veuillez afficher les tags corrects de `premiere_phrase_test`, puis comparez-les visuellement les tags trouvés automatiquement au (3a).  Quelles différences trouvez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:46:55.664173Z",
     "start_time": "2025-03-16T18:46:55.660286Z"
    }
   },
   "source": [
    "print(\"Tags corrects de la première phrase :\")\n",
    "\n",
    "res = [f\"{token.text} [{token.pos_}]\" for token in premiere_phrase_test]\n",
    "print(' '.join(res))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags corrects de la première phrase :\n",
      "Je [PRON] sens [VERB] qu' [SCONJ] entre [ADP] ça [PRON] et [CCONJ] les [DET] films [NOUN] de [ADP] médecins [NOUN] et [CCONJ] scientifiques [NOUN] fous [ADJ] que [PRON] nous [PRON] avons [AUX] déjà [ADV] vus [VERB] , [PUNCT] nous [PRON] pourrions [VERB] emprunter [VERB] un [DET] autre [ADJ] chemin [NOUN] pour [ADP] l' [DET] origine [NOUN] . [PUNCT]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> En observant les tags corrects ainsi que ceux obtenus avec la pipeline, on observe une unique différence. Le mot `fous` est considéré comme étant un pronom par la pipeline alors que celui-ci est un adjectif. La pipeline a donc proposé un seul mauvais tag sur les 29 mots à tagger."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:46:55.713665Z",
     "start_time": "2025-03-16T18:46:55.711031Z"
    }
   },
   "source": [
    "from spacy.scorer import Scorer\n",
    "from spacy.training import Example"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:46:55.889264Z",
     "start_time": "2025-03-16T18:46:55.758011Z"
    }
   },
   "source": [
    "scorer = Scorer()"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3c.** Au lieu de compter manuellement combien de tags sont différents entre la référence et le résultat de la pipeline `nlp`, vous allez utiliser la classe `Scorer` de spaCy.  Une instance de cette classe permet de calculer les scores d'une liste d'objets de type `Exemple`, en fonction des annotations disponibles dans les objets.  Un objet de type `Exemple` contient deux objets de type `Doc`, l'un avec les annotations correctes et l'autre avec les annotations produites par une pipeline.  La [documentation de la méthode](https://spacy.io/api/scorer#score) `Scorer.score(..)` vous sera utile. \n",
    "\n",
    "* Veuillez calculer la justesse (*accuracy*) du *POS tagging* de `premiere_phrase_test`. \n",
    "* Veuillez justifier la valeur du score obtenu en utilisant votre réponse du (3b)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:46:55.904698Z",
     "start_time": "2025-03-16T18:46:55.898686Z"
    }
   },
   "source": [
    "score = scorer.score([Example(sentence_doc, premiere_phrase_test.as_doc())])\n",
    "print(f\"POS Accuracy: {(score.get('pos_acc')*100):.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Accuracy: 96.55%\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Le scorer indique une précision du tagging à 96.55%, ce qui correspond à celui que l'on peut caluler au point 3b. En effet, un seul token a mal été taggé sur un ensemble de 29 tokens, ce qui donne une précision de 28/29 = 0.9655, soit 96.55%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3d.** Veuillez calculer la précision du *POS tagging* de la pipeline `nlp` sur toutes les données de test présentes dans `test_data`.  Comment se compare le score obtenu avec celui mentionné [dans la documentation](https://spacy.io/models/fr#fr_core_news_sm) du modèle `fr_core_news_sm` ?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:46:58.263419Z",
     "start_time": "2025-03-16T18:46:55.958428Z"
    }
   },
   "source": [
    "sentences = [sentence for doc in test_data.get_docs(nlp.vocab) for sentence in doc.sents]\n",
    "score_sentences = scorer.score(\n",
    "    [\n",
    "        Example(nlp(sentence.as_doc().copy()), sentence.as_doc())\n",
    "        for sentence in sentences\n",
    "    ]\n",
    ")\n",
    "print(f\"POS Accuracy over sentences: {(score_sentences.get('pos_acc') * 100):.2f}%\")\n",
    "\n",
    "\n",
    "score_docs = scorer.score(\n",
    "    [Example(nlp(doc.copy()), doc)\n",
    "     for doc in test_data.get_docs(nlp.vocab)]\n",
    ")\n",
    "print(f\"POS Accuracy over whole documents: {(score_docs.get('pos_acc') * 100):.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Accuracy over sentences: 91.73%\n",
      "POS Accuracy over whole documents: 91.77%\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nous avons d'abord testé la précision phrase par phrase, sans tenir compte du contexte, comme dans les exercices précédents. La précision obtenue est de 91.73%. Ensuite, nous avons comparé cette précision avec celle obtenue en traitant chaque document dans son ensemble, en ne séparant pas les phrases d'un même document. Le résultat est similaire, avec un score légèrement plus élevé lorsque les documents sont pris dans leur ensemble (91.77%). Cela nous semble logique étant donné que dans certains cas le contexte peut aider à déterminer le bon tag.\n",
    "> \n",
    "> La documentation du modèle indique une précision attendue de 0.96 (96%) pour le tagging, supérieure à nos résultats. Cela pourrait s'expliquer par le fait que le modèle fourni a été entraîné sur un jeu de données qqui n'est pas contextuellement similaire aux données de test ou par une présence plus importante de mots difficile à classifier dans celui-ci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entraîner puis évaluer un nouveau POS tagger français dans spaCy\n",
    "\n",
    "Le but de cette partie est d'entraîner une pipeline spaCy pour le français sur les données de `fr-ud-train.conllu`, puis de comparer le modèle obtenu avec le modèle prêt-à-l'emploi testé au point précédent.  Les [instructions d'entraînement](https://spacy.io/usage/training#quickstart) de spaCy vous montrent comment entraîner une pipeline avec un POS tagger.\n",
    "\n",
    "**4a.** Paramétrage de l'entraînement :\n",
    "* générez un fichier de départ grâce à [l'interface web](https://spacy.io/usage/training#quickstart), en indiquant que vous voulez seulement un POS tagger dans la pipeline ;\n",
    "* sauvegardez le code généré par spaCy dans un fichier local `base_config.cfg` ;\n",
    "* générez un fichier `config.cfg` sur votre ordinateur en exécutant la ligne de commande suivante. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:47:02.384767Z",
     "start_time": "2025-03-16T18:46:58.272673Z"
    }
   },
   "source": [
    "!python -m spacy init fill-config base_config.cfg config.cfg"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;2m✔ Auto-filled config with all values\u001B[0m\r\n",
      "\u001B[38;5;2m✔ Saved config\u001B[0m\r\n",
      "config.cfg\r\n",
      "You can now add your data and train your pipeline:\r\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\r\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, veuillez effectuer l'entraînement avec la ligne de commande suivante.  Faites plusieurs essais, d'abord avec un petit nombre d'époques, pour estimer le temps nécessaire et observer les messages affichés.  Puis augmentez progressivement le nombre d'époques.  Quel est le critère qui vous permet de décider que vous avez un nombre suffisant d'époques ?  Dans quel dossier se trouve le meilleur modèle ?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:52:47.938448Z",
     "start_time": "2025-03-16T18:47:02.403368Z"
    }
   },
   "source": [
    "!python -m spacy train config.cfg \\\n",
    "  --output ./myPOStagger-5epoch \\\n",
    "  --paths.train ./spacy_data/fr-ud-train.spacy \\\n",
    "  --paths.dev ./spacy_data/fr-ud-dev.spacy \\\n",
    "  --verbose"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-16 19:47:03,444] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\r\n",
      "\u001B[38;5;4mℹ Saving to output directory: myPOStagger-5epoch\u001B[0m\r\n",
      "\u001B[38;5;4mℹ Using CPU\u001B[0m\r\n",
      "\u001B[1m\r\n",
      "=========================== Initializing pipeline ===========================\u001B[0m\r\n",
      "[2025-03-16 19:47:07,148] [INFO] Set up nlp object from config\r\n",
      "[2025-03-16 19:47:07,160] [DEBUG] Loading corpus from path: spacy_data/fr-ud-dev.spacy\r\n",
      "[2025-03-16 19:47:07,161] [DEBUG] Loading corpus from path: spacy_data/fr-ud-train.spacy\r\n",
      "[2025-03-16 19:47:07,161] [INFO] Pipeline: ['tok2vec', 'tagger']\r\n",
      "[2025-03-16 19:47:07,164] [INFO] Created vocabulary\r\n",
      "[2025-03-16 19:47:07,164] [INFO] Finished initializing nlp object\r\n",
      "[2025-03-16 19:47:22,647] [INFO] Initialized pipeline components: ['tok2vec', 'tagger']\r\n",
      "\u001B[38;5;2m✔ Initialized pipeline\u001B[0m\r\n",
      "\u001B[1m\r\n",
      "============================= Training pipeline =============================\u001B[0m\r\n",
      "[2025-03-16 19:47:22,659] [DEBUG] Loading corpus from path: spacy_data/fr-ud-dev.spacy\r\n",
      "[2025-03-16 19:47:22,661] [DEBUG] Loading corpus from path: spacy_data/fr-ud-train.spacy\r\n",
      "[2025-03-16 19:47:22,671] [DEBUG] Removed existing output directory: myPOStagger-5epoch/model-best\r\n",
      "[2025-03-16 19:47:22,672] [DEBUG] Removed existing output directory: myPOStagger-5epoch/model-last\r\n",
      "\u001B[38;5;4mℹ Pipeline: ['tok2vec', 'tagger']\u001B[0m\r\n",
      "\u001B[38;5;4mℹ Initial learn rate: 0.001\u001B[0m\r\n",
      "E    #       LOSS TOK2VEC  LOSS TAGGER  TAG_ACC  SCORE \r\n",
      "---  ------  ------------  -----------  -------  ------\r\n",
      "  0       0          0.00       211.77    36.34    0.36\r\n",
      "  0     200        315.78     10403.58    90.38    0.90\r\n",
      "  0     400        287.59      4500.03    91.63    0.92\r\n",
      "  0     600        222.64      3447.72    92.14    0.92\r\n",
      "  0     800        217.77      3472.31    92.58    0.93\r\n",
      "  0    1000        190.27      3012.89    92.61    0.93\r\n",
      "  0    1200        184.36      2921.70    92.90    0.93\r\n",
      "  0    1400        171.64      2746.66    93.02    0.93\r\n",
      "  1    1600        147.59      2232.49    93.10    0.93\r\n",
      "  1    1800        140.07      2025.26    93.13    0.93\r\n",
      "  1    2000        155.89      2274.63    93.22    0.93\r\n",
      "  1    2200        147.96      2143.40    93.26    0.93\r\n",
      "  1    2400        148.56      2133.26    93.39    0.93\r\n",
      "  1    2600        150.10      2141.56    93.20    0.93\r\n",
      "  1    2800        152.66      2182.57    93.46    0.93\r\n",
      "  2    3000        131.79      1809.59    93.36    0.93\r\n",
      "  2    3200        122.10      1576.16    93.27    0.93\r\n",
      "  2    3400        134.89      1725.80    93.49    0.93\r\n",
      "  2    3600        131.55      1643.79    93.47    0.93\r\n",
      "  2    3800        126.99      1564.77    93.42    0.93\r\n",
      "  2    4000        133.31      1665.34    93.42    0.93\r\n",
      "  2    4200        141.77      1755.45    93.55    0.94\r\n",
      "  3    4400        139.83      1696.87    93.53    0.94\r\n",
      "  3    4600        101.02      1149.63    93.57    0.94\r\n",
      "  3    4800        116.95      1310.59    93.61    0.94\r\n",
      "  3    5000        127.54      1422.67    93.49    0.93\r\n",
      "  3    5200        126.94      1419.86    93.60    0.94\r\n",
      "  3    5400        125.46      1401.61    93.52    0.94\r\n",
      "  3    5600        123.99      1371.43    93.57    0.94\r\n",
      "  3    5800        133.30      1439.02    93.69    0.94\r\n",
      "  4    6000        111.11      1149.63    93.63    0.94\r\n",
      "  4    6200        105.39      1048.16    93.56    0.94\r\n",
      "  4    6400        112.71      1114.82    93.63    0.94\r\n",
      "  4    6600        121.72      1216.46    93.67    0.94\r\n",
      "  4    6800        121.94      1222.12    93.68    0.94\r\n",
      "  4    7000        121.71      1226.58    93.60    0.94\r\n",
      "  4    7200        114.93      1125.15    93.61    0.94\r\n",
      "  5    7400        106.20      1017.87    93.62    0.94\r\n",
      "\u001B[38;5;2m✔ Saved pipeline to output directory\u001B[0m\r\n",
      "myPOStagger-5epoch/model-last\r\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nous avons tenté plusieur valeurs pour le nombre d'epoques, avant d'afficher une partie des résultats obtenus ci-dessous.\n",
    "\n",
    "Avec `max_epoch=1`\n",
    "\n",
    "```\n",
    "E    #       LOSS TOK2VEC  LOSS TAGGER  TAG_ACC  SCORE\n",
    "---  ------  ------------  -----------  -------  ------\n",
    "  0       0          0.00       211.77    36.34    0.36\n",
    "  0     200        315.78     10403.58    90.38    0.90\n",
    "  0     400        287.59      4500.03    91.63    0.92\n",
    "  0     600        222.64      3447.72    92.14    0.92\n",
    "  0     800        217.77      3472.31    92.58    0.93\n",
    "  0    1000        190.27      3012.89    92.61    0.93\n",
    "  0    1200        184.36      2921.70    92.90    0.93\n",
    "  0    1400        171.64      2746.66    93.02    0.93\n",
    "```\n",
    "\n",
    "Avec `max_epoch=5`\n",
    "\n",
    "```\n",
    "E    #       LOSS TOK2VEC  LOSS TAGGER  TAG_ACC  SCORE\n",
    "---  ------  ------------  -----------  -------  ------\n",
    "  0       0          0.00       211.77    36.34    0.36\n",
    "  0     200        315.78     10403.58    90.38    0.90\n",
    "  0     400        287.59      4500.03    91.63    0.92\n",
    "  0     600        222.64      3447.72    92.14    0.92\n",
    "  0     800        217.77      3472.31    92.58    0.93\n",
    "  0    1000        190.27      3012.89    92.61    0.93\n",
    "  0    1200        184.36      2921.70    92.90    0.93\n",
    "  0    1400        171.64      2746.66    93.02    0.93\n",
    "  1    1600        147.59      2232.49    93.10    0.93\n",
    "  1    1800        140.07      2025.26    93.13    0.93\n",
    "  1    2000        155.89      2274.63    93.22    0.93\n",
    "  1    2200        147.96      2143.40    93.26    0.93\n",
    "  1    2400        148.56      2133.26    93.39    0.93\n",
    "  1    2600        150.10      2141.56    93.20    0.93\n",
    "  1    2800        152.66      2182.57    93.46    0.93\n",
    "  2    3000        131.79      1809.59    93.36    0.93\n",
    "  2    3200        122.10      1576.16    93.27    0.93\n",
    "  2    3400        134.89      1725.80    93.49    0.93\n",
    "  2    3600        131.55      1643.79    93.47    0.93\n",
    "  2    3800        126.99      1564.77    93.42    0.93\n",
    "  2    4000        133.31      1665.34    93.42    0.93\n",
    "  2    4200        141.77      1755.45    93.55    0.94\n",
    "  3    4400        139.83      1696.87    93.53    0.94\n",
    "  3    4600        101.02      1149.63    93.57    0.94\n",
    "  3    4800        116.95      1310.59    93.61    0.94\n",
    "  3    5000        127.54      1422.67    93.49    0.93\n",
    "  3    5200        126.94      1419.86    93.60    0.94\n",
    "  3    5400        125.46      1401.61    93.52    0.94\n",
    "  3    5600        123.99      1371.43    93.57    0.94\n",
    "  3    5800        133.30      1439.02    93.69    0.94\n",
    "  4    6000        111.11      1149.63    93.63    0.94\n",
    "  4    6200        105.39      1048.16    93.56    0.94\n",
    "  4    6400        112.71      1114.82    93.63    0.94\n",
    "  4    6600        121.72      1216.46    93.67    0.94\n",
    "  4    6800        121.94      1222.12    93.68    0.94\n",
    "  4    7000        121.71      1226.58    93.60    0.94\n",
    "  4    7200        114.93      1125.15    93.61    0.94\n",
    "```\n",
    "\n",
    "Avec `max_epoch=0` (arrêt automatique après 1600 étapes sans amélioration, configuration par défaut)\n",
    "\n",
    "```\n",
    "E    #       LOSS TOK2VEC  LOSS TAGGER  TAG_ACC  SCORE\n",
    "---  ------  ------------  -----------  -------  ------\n",
    "  0       0          0.00       211.77    36.34    0.36\n",
    "  0     200        315.78     10403.58    90.38    0.90\n",
    "  0     400        287.59      4500.03    91.63    0.92\n",
    "  0     600        222.64      3447.72    92.14    0.92\n",
    "  0     800        217.77      3472.31    92.58    0.93\n",
    "  0    1000        190.27      3012.89    92.61    0.93\n",
    "  0    1200        184.36      2921.70    92.90    0.93\n",
    "  0    1400        171.64      2746.66    93.02    0.93\n",
    "  1    1600        147.59      2232.49    93.10    0.93\n",
    "  1    1800        140.07      2025.26    93.13    0.93\n",
    "  1    2000        155.89      2274.63    93.22    0.93\n",
    "  1    2200        147.96      2143.40    93.26    0.93\n",
    "  1    2400        148.56      2133.26    93.39    0.93\n",
    "  1    2600        150.10      2141.56    93.20    0.93\n",
    "  1    2800        152.66      2182.57    93.46    0.93\n",
    "  2    3000        131.79      1809.59    93.36    0.93\n",
    "  2    3200        122.10      1576.16    93.27    0.93\n",
    "  2    3400        134.89      1725.80    93.49    0.93\n",
    "  2    3600        131.55      1643.79    93.47    0.93\n",
    "  2    3800        126.99      1564.77    93.42    0.93\n",
    "  2    4000        133.31      1665.34    93.42    0.93\n",
    "  2    4200        141.77      1755.45    93.55    0.94\n",
    "  3    4400        139.83      1696.87    93.53    0.94\n",
    "  3    4600        101.02      1149.63    93.57    0.94\n",
    "  3    4800        116.95      1310.59    93.61    0.94\n",
    "  3    5000        127.54      1422.67    93.49    0.93\n",
    "  3    5200        126.94      1419.86    93.60    0.94\n",
    "  3    5400        125.46      1401.61    93.52    0.94\n",
    "  3    5600        123.99      1371.43    93.57    0.94\n",
    "  3    5800        133.30      1439.02    93.69    0.94\n",
    "  4    6000        111.11      1149.63    93.63    0.94\n",
    "  4    6200        105.39      1048.16    93.56    0.94\n",
    "  4    6400        112.71      1114.82    93.63    0.94\n",
    "  4    6600        121.72      1216.46    93.67    0.94\n",
    "  4    6800        121.94      1222.12    93.68    0.94\n",
    "  4    7000        121.71      1226.58    93.60    0.94\n",
    "  4    7200        114.93      1125.15    93.61    0.94\n",
    "  5    7400        106.20      1017.87    93.62    0.94\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> En observant la diminution de la perte et l'augmentation de la précision, il est possible de déterminer si le nombre d'epochs est suffisant. Une augmentation de la perte ou une précision stagnante ou décroissante montrent que le nombre optimal d'epochs a été dépassé. La configuration par défaut utilise un nombre illimité d'epoch (`max_epoch=0`) mais arrete si le modèle stagne depuis 1600 étapes. Cela permet d'éviter de devoir déterminer le nombre d'épochs de façon empirique.\n",
    "> Dans le cas présent, le modèle s'arrête après 5 epochs, offrant un score de 0.94 et une précision du tagger de 93.61%. Ce modèle étant le meilleur de la série d'entrainement, est enregistré dans le sous-dossier `model-best` du dossier cible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4b.**  Veuillez charger le meilleur modèle (pipeline) dans la variable `nlp2` et afficher la *POS tagging accuracy* sur le corpus de test.  Le composant de la pipeline étant un *POS tagger*, vous devrez évaluer la propriété *tag_acc*. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T19:03:25.961591Z",
     "start_time": "2025-03-16T19:03:25.094366Z"
    }
   },
   "source": [
    "nlp2 = spacy.load(\"./myPOStagger-5epoch/model-best\")\n",
    "score = scorer.score([Example(nlp2(doc.text), doc) for doc in list(test_data.get_docs(nlp.vocab))])\n",
    "\n",
    "print(f\"POS Accuracy: {(score.get('tag_acc')*100):.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Accuracy: 93.42%\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraîner puis évaluer un POS tagger pour le français dans NLTK\n",
    "\n",
    "Le but de cette partie est d'utiliser le POS tagger appelé *Averaged Perceptron* fourni par NLTK, en l'entraînant pour le français sur les mêmes données que ci-dessus, importées cette fois-ci avec NLTK.  Pour une introduction au POS tagging avec NLTK, voir le [Chapitre 5.1 du livre NLTK](http://www.nltk.org/book/ch05.html).\n",
    "\n",
    "Remarques :\n",
    "* pour l'anglais, des taggers pré-entraînés sont disponibles dans NLTK ;\n",
    "* pour appliquer un tagger existant, on écrit `nltk.pos_tag(sentence)` où `sentence` est une liste de tokens et on obtient des paires (token, TAG) ;\n",
    "* l'implémentation de *Averaged Perceptron* a été faite par [Mathew Honnibal de Explosion.AI](https://explosion.ai/blog/part-of-speech-pos-tagger-in-python), la société qui a créé spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5a.** Veuillez charger les données d'entraînement et celles de test grâce à la classe `ConllCorpusReader` de NLTK.  [La documentation de cette classe](https://www.nltk.org/api/nltk.corpus.reader.conll.html#nltk.corpus.reader.conll.ConllCorpusReader) vous montrera comment indiquer les colonnes qui contiennent les tokens ('words') et les tags corrects ('pos').  Une fois les données chargées dans une variable, vous pouvez accéder aux phrases et aux tags avec la méthode `.tagged_sents()`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:52:49.304687Z",
     "start_time": "2025-03-16T18:52:48.997541Z"
    }
   },
   "source": [
    "from nltk.corpus.reader.conll import ConllCorpusReader"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:52:49.321292Z",
     "start_time": "2025-03-16T18:52:49.315850Z"
    }
   },
   "source": [
    "train = ConllCorpusReader(\"./input_data\", \"fr-ud-train.conllu\", (\"ignore\", \"words\", \"ignore\", \"pos\"), separator=\"\\t\")\n",
    "test = ConllCorpusReader(\"./input_data\", \"fr-ud-test.conllu\", (\"ignore\", \"words\", \"ignore\", \"pos\"), separator=\"\\t\")\n",
    "\n",
    "sentences_train = train.tagged_sents()\n",
    "sentences_test = test.tagged_sents()"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5b.** Pour entraîner un POS tagger du type Averaged Perceptron, vous utiliserez le sous-module `nltk.tag.perceptron` du [module NLTK contenant les taggers](http://www.nltk.org/api/nltk.tag.html).  Les fonctions d'entraînement et de test sont documentées dans ce module.  Après l'entraînement, le réseau de neurones est enregistré dans un fichier `.pickle`, qui est écrasé à chaque entraînement si vous n'en faites pas une copie.  On peut également lire un fichier `.pickle` dans un tagger.\n",
    "\n",
    "Veuillez écrire le code pour entraîner le POS tagger sur les données d'entraînement.  Comme au (4), pensez augmenter graduellement le nombre d'époques (appelées 'itérations' dans NLTK).\n",
    "\n",
    "Combien de temps prend l'entraînement ?  Quelle est la taille du fichier enregistré ?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:52:51.782261Z",
     "start_time": "2025-03-16T18:52:50.478316Z"
    }
   },
   "source": [
    "# NOTE: NLTK a retiré la gestion des fichiers pickle pour un stockage sous format JSON il y a 8 mois,\n",
    "#       il n'est plus possible de sauvegarder directement le fichier pickle du a un bug dans la librairie,\n",
    "#       on doit donc passer par la méthode save_to_json pour sauvegarder le modèle en format JSON,\n",
    "#       mais elle ne marche pas. On force le requirement à une version qui fonctionne...\n",
    "!python -m pip install 'nltk==3.8.1'\n",
    "\n",
    "import os\n",
    "#import nltk\n",
    "#nltk.download('averaged_perceptron_tagger') # à exécuter la première fois\n",
    "from nltk.tag.perceptron import PerceptronTagger # Non fonctionnel pour nltk >= 3.8.2"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk==3.8.1 in /home/loich/miniconda3/envs/pw2/lib/python3.12/site-packages (3.8.1)\r\n",
      "Requirement already satisfied: click in /home/loich/miniconda3/envs/pw2/lib/python3.12/site-packages (from nltk==3.8.1) (8.1.8)\r\n",
      "Requirement already satisfied: joblib in /home/loich/miniconda3/envs/pw2/lib/python3.12/site-packages (from nltk==3.8.1) (1.4.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/loich/miniconda3/envs/pw2/lib/python3.12/site-packages (from nltk==3.8.1) (2024.11.6)\r\n",
      "Requirement already satisfied: tqdm in /home/loich/miniconda3/envs/pw2/lib/python3.12/site-packages (from nltk==3.8.1) (4.67.1)\r\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T19:03:24.587111Z",
     "start_time": "2025-03-16T18:55:50.950094Z"
    }
   },
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "for n in range(2, 12):\n",
    "    ptagger = PerceptronTagger(load=False) # Used to have an untrained tagger each time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model_path = f\"./nltkPOStagger/model-{n}epoch.pickle\"\n",
    "    ptagger.train(sentences_train, model_path, nr_iter=n)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    training_time = end_time - start_time\n",
    "    accuracy = ptagger.accuracy(sentences_test)\n",
    "    file_size = os.path.getsize(model_path) if os.path.exists(model_path) else 0\n",
    "\n",
    "    print(f\"Epochs: {n}\\tAccuracy: {accuracy:.4f}\\tTraining Time: {training_time:.2f} seconds\\tFile Size: {file_size / 1024:.2f} KB\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2\tAccuracy: 0.9565\tTraining Time: 20.66 seconds\tFile Size: 5057.80 KB\n",
      "Epochs: 3\tAccuracy: 0.9575\tTraining Time: 28.18 seconds\tFile Size: 5581.17 KB\n",
      "Epochs: 4\tAccuracy: 0.9581\tTraining Time: 31.71 seconds\tFile Size: 5921.65 KB\n",
      "Epochs: 5\tAccuracy: 0.9582\tTraining Time: 35.53 seconds\tFile Size: 6152.87 KB\n",
      "Epochs: 6\tAccuracy: 0.9599\tTraining Time: 42.36 seconds\tFile Size: 6308.92 KB\n",
      "Epochs: 7\tAccuracy: 0.9593\tTraining Time: 54.07 seconds\tFile Size: 6433.43 KB\n",
      "Epochs: 8\tAccuracy: 0.9602\tTraining Time: 51.50 seconds\tFile Size: 6543.33 KB\n",
      "Epochs: 9\tAccuracy: 0.9604\tTraining Time: 56.30 seconds\tFile Size: 6628.68 KB\n",
      "Epochs: 10\tAccuracy: 0.9610\tTraining Time: 63.14 seconds\tFile Size: 6696.56 KB\n",
      "Epochs: 11\tAccuracy: 0.9602\tTraining Time: 67.31 seconds\tFile Size: 6761.97 KB\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T19:03:24.751606Z",
     "start_time": "2025-03-16T19:03:24.649508Z"
    }
   },
   "source": [
    "ptagger.load(\"./nltkPOStagger/model-10epoch.pickle\")"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Après avoir testé manuellement certaines valeurs d'epochs, nous avons décidé de tester de manière plus exhaustive. Nous avons donc testé les valeurs pour le nombre d'epochs, allant de 2 à 15, dont les résultats sont visibles ci-dessus. Il est important de prendre en compte le fait que l'entrainement peut offrir des précisions qui varient d'une exécution à l'autre. Bien que le modèle entrainé sur 14 epochs soient légèrement plus performant que le modèle entrainé sur 10 epochs lors de cette execution, nous avons sélectionné le second modèle. Nous avons choisis ce modèle car c'est celui qui revenait le plus souvent comme étant le meilleur lors de nos différentes executions de la cellule.\n",
    "> \n",
    "> En observant les résultats, il est possible de remarquer plusieurs choses intéressantes : le temps d'entrainement augmente de manière presque linéaire et la taille du fichier généré semble augmenter selon une courbe logarithmique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5c.** Veuillez évaluer le tagger sur les données de test et afficher le taux de correction."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T19:03:25.025903Z",
     "start_time": "2025-03-16T19:03:24.757061Z"
    }
   },
   "source": [
    "print(f\"POS Accuracy: {(ptagger.accuracy(sentences_test)*100):.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Accuracy: 96.10%\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "Veuillez remplir le tableau suivant avec les scores obtenus et discuter brièvement comment se comparent les trois POS taggers sur ces données de test.\n",
    "\n",
    "| spaCy (partie 3) | spaCy (partie 4) | NLTK (partie 5) | \n",
    "|------------------|------------------|-----------------|\n",
    "| tagger fourni    | tagger entraîné  | tagger entraîné |\n",
    "| 0.9177           | 0.9342           | 0.9610          |\n",
    "\n",
    "> En observant les scores obtenus, on constate que le tagger entrainé avec NLTK obtient les meilleurs résultats, suivit par le tagger entrainé avec spaCy. Sans grande surprise, le tagger fourni par spaCy obtient de moins bons résultats que les deux autres modèles entrainés par nos soins. Cela provient sans doute du fait que le modèle fourni par spaCy a été entrainé sur un jeu de données comportant des différences notables avec les jeux de données utilisées dans ce laboratoire.\n",
    "> \n",
    "> En effet, en entrainant les modèles à l'aide des données fournies pour ce laboratoire, nous sommes capables de créer un tagger qui généralise mieux la sémantique, le contexte et le sens du corpus étudié. Ceux-ci offrent alors de meilleurs résultats lors de l'évaluation du jeu de test puisque ce jeu de données est proche dans sa composition de celui d'entrainement. Le modèle préentrainé ne pouvant pas bénéficier d'un entrainement lui permettant de généraliser des particularités de nos données, celui-ci se retrouve limité en termes de précision lors de l'évaluation du jeu de test. Il reste important de préciser qu'il ne faut pas entrainer un modèle sur un corpus trop spécifique, au risque de tomber dans de l'overfitting et de se retrouver avec un modèle incapable de généraliser lorsqu'il se retrouve face à des données inconnues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fin du Labo.** Veuillez nettoyer ce notebook en gardant seulement les résultats désirés, l'enregistrer, et le soumettre comme devoir sur Cyberlearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
